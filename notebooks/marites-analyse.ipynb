{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6757cd5",
   "metadata": {},
   "source": [
    "# Marites Analyse\n",
    "\n",
    "## Overview\n",
    "Contains the logic for the analyse function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7da1212e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import complete.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import re\n",
    "import boto3\n",
    "from io import StringIO, BytesIO\n",
    "from uuid import uuid4\n",
    "import tarfile\n",
    "from tempfile import TemporaryDirectory\n",
    "\n",
    "load_dotenv()\n",
    "print(\"Import complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96f6fa42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2232610a-3a1e-4d49-b79e-1c7c3aba0ec2\n"
     ]
    }
   ],
   "source": [
    "max_twitter_posts = 100\n",
    "max_following = 250\n",
    "token = os.environ.get(\"BEARER_TOKEN\")\n",
    "test_username = 'JoseRizal619'\n",
    "\n",
    "region = 'ap-southeast-2'\n",
    "language_code = 'en'\n",
    "input_bucket = 'marites-comprehend-input'\n",
    "output_bucket = 'marites-comprehend-output'\n",
    "data_access_role_arn = os.environ.get(\"DATA_ACCESS_ROLE\")\n",
    "input_doc_format = 'ONE_DOC_PER_LINE'\n",
    "\n",
    "tg_input_folder = 'tigergraph' \n",
    "comprehend_input_folder = 'comprehend'\n",
    "\n",
    "session_id = uuid4()\n",
    "print(session_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e066e940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twitter Functions\n",
    "\n",
    "search_url = \"https://api.twitter.com/2/tweets/search/recent\"\n",
    "following_url = \"https://api.twitter.com/2/users/{}/following\"\n",
    "lookup_username_url = \"https://api.twitter.com/2/users/by/username/{}\"\n",
    "\n",
    "\n",
    "def bearer_oauth(r):\n",
    "    \"\"\"\n",
    "    Method required by bearer token authentication.\n",
    "    \"\"\"\n",
    "    r.headers[\"Authorization\"] = f\"Bearer {token}\"\n",
    "    return r\n",
    "\n",
    "\n",
    "def fetch_user_by_username(username):\n",
    "    url = lookup_username_url.format(username)\n",
    "    response = requests.get(url, auth=bearer_oauth)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(response.status_code, response.text)\n",
    "    json_res = response.json()\n",
    "    return json_res['data']\n",
    "\n",
    "def map_tweets_to_post(raw_data):\n",
    "    if 'data' not in raw_data:\n",
    "        return []\n",
    "\n",
    "    tweets = raw_data['data']\n",
    "    username = raw_data['includes']['users'][0]['username']\n",
    "    ref_tweets = { tweet['id']: tweet['text'] for tweet in raw_data['includes']['tweets'] } if 'includes' in raw_data and 'tweets' in raw_data['includes'] else {}\n",
    "    \n",
    "    results = []\n",
    "    for t in tweets:\n",
    "        post = { \n",
    "            'tweet_id': t['id'],\n",
    "            'username': username,\n",
    "            'created_at': t['created_at']\n",
    "        }\n",
    "        if 'referenced_tweets' in t:\n",
    "            combined_text = []\n",
    "            for rt in t['referenced_tweets']:\n",
    "                rt_id = rt['id']\n",
    "                if rt_id in ref_tweets:\n",
    "                    rt_text = ref_tweets[rt_id]\n",
    "                    combined_text.append(rt_text)\n",
    "            post['text'] = ' '.join(combined_text)\n",
    "        else:\n",
    "            post['text'] = t['text']\n",
    "\n",
    "        results.append(post)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def fetch_tweets_by_username(username):\n",
    "    params = {\n",
    "        \"query\": \"from:{} -is:reply\".format(username),\n",
    "        \"max_results\": max_twitter_posts,\n",
    "        \"expansions\": \"referenced_tweets.id,author_id\",\n",
    "        \"tweet.fields\": \"created_at\",\n",
    "        \"user.fields\": \"username\"\n",
    "    }\n",
    "    response = requests.get(search_url, auth=bearer_oauth, params=params)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(response.status_code, response.text)\n",
    "    data = response.json()\n",
    "    return map_tweets_to_post(data)\n",
    "\n",
    "def fetch_following(user_id):\n",
    "    url = following_url.format(user_id)\n",
    "    params = {\n",
    "        'max_results': max_following\n",
    "    }\n",
    "    response = requests.get(url, auth=bearer_oauth, params=params)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(response.status_code, response.text)\n",
    "    json_res = response.json()\n",
    "    return json_res['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f62234e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twitter data extraction\n",
    "\n",
    "def get_user_tweets(users_to_search):\n",
    "    processed = 0\n",
    "    all_tweets = []\n",
    "    for user in users_to_search:\n",
    "        user_tweets = fetch_tweets_by_username(user)\n",
    "        processed += 1\n",
    "        all_tweets.extend(user_tweets)\n",
    "        progress = round((processed / len(users_to_search)) * 100, 2)\n",
    "        print(\"Processed {}/{} users ({}%)\".format(processed, len(users_to_search), progress))\n",
    "    user_tweets = pd.DataFrame(all_tweets)\n",
    "    return user_tweets\n",
    "\n",
    "def get_user_following_map(user, following):\n",
    "    date = datetime.now().strftime(\"%m-%d-%y\")\n",
    "    username = user['username']\n",
    "    follow_names = list(map(lambda x: x['username'], following))\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        'user': [username] * len(following),\n",
    "        'following': follow_names,\n",
    "        'date': [date] * len(following)\n",
    "    })\n",
    "\n",
    "\n",
    "def clean_posts(data):\n",
    "    user_tweets = data\n",
    "    \n",
    "    if user_tweets.empty:\n",
    "        return []\n",
    "    \n",
    "    # Clean up the links from the text (they're useless to us)\n",
    "    user_tweets['text'] = user_tweets['text'].apply(lambda x: re.split('https:\\/\\/.*', str(x))[0])\n",
    "\n",
    "    # Remove all emojis\n",
    "    user_tweets = user_tweets.astype(str).apply(lambda x: x.str.encode('ascii', 'ignore').str.decode('ascii'))\n",
    "\n",
    "    # Remove blank tweets\n",
    "    user_tweets = user_tweets[user_tweets.text.str.strip().str.len() != 0]\n",
    "\n",
    "    # Ensure that all text is in a single line\n",
    "    user_tweets.text = user_tweets.text.str.replace('\\n', ' ');\n",
    "    user_tweets.text = user_tweets.text.str.replace('\\r', ' ');\n",
    "    \n",
    "    return user_tweets\n",
    "\n",
    "def extract_twitter_data(username):\n",
    "    users_list = []\n",
    "    user = fetch_user_by_username(username)\n",
    "    user_following = fetch_following(user['id'])\n",
    "\n",
    "    users_list.append(user)\n",
    "    users_list.extend(user_following)\n",
    "    \n",
    "    users_to_search = list(map(lambda x: x['username'], users_list))\n",
    "    \n",
    "    posts_df = get_user_tweets(users_to_search)\n",
    "    following_df = get_user_following_map(user, user_following) # user -> following edges\n",
    "    users_df = pd.DataFrame(users_list) # users vertex\n",
    "    \n",
    "    print(posts_df)\n",
    "    \n",
    "    return {\n",
    "        'posts': clean_posts(posts_df),\n",
    "        'following': following_df,\n",
    "        'users': users_df\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f7f90f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload to S3 functions\n",
    "\n",
    "def upload_text_to_s3(data, bucket_name, file_name):\n",
    "    text_buffer = StringIO()\n",
    "    data.text.to_csv(text_buffer, sep=' ', index=False, header=False)\n",
    "    s3_resource = boto3.resource('s3')\n",
    "    return s3_resource.Object(bucket_name, '{}.txt'.format(file_name)).put(Body=text_buffer.getvalue())\n",
    "\n",
    "def upload_frames_to_s3(tar_filename, bucket_name, frame_dict):\n",
    "    tar_buffer = BytesIO()\n",
    "    \n",
    "    # Create a tarfile into which frames can be added\n",
    "    with tarfile.open(fileobj=tar_buffer, mode='w:gz') as tfo:\n",
    "        \n",
    "        # Loop over all dataframes to be saved\n",
    "        for file_name, df in frame_dict.items():\n",
    "            \n",
    "            # Compute the full path of the output file within the archive\n",
    "            archive_name = os.path.join('output', file_name)\n",
    "            \n",
    "            # Create a temporary directory for packaging into a tar_file\n",
    "            with TemporaryDirectory(prefix='rev_processing__') as temp_dir:\n",
    "                \n",
    "                # Write a csv dump of the dataframe to a temporary file\n",
    "                temp_file_name = os.path.join(temp_dir, archive_name)\n",
    "                os.makedirs(os.path.dirname(temp_file_name), exist_ok=True)\n",
    "                df.to_csv(temp_file_name, index=False)\n",
    "                \n",
    "                # Add the temp file to the tarfile\n",
    "                tfo.add(temp_file_name, arcname=archive_name)\n",
    "    \n",
    "    # Upload to S3\n",
    "    s3_resource = boto3.resource('s3')\n",
    "    return s3_resource.Object(bucket_name, f'{tar_filename}.tar.gz').put(Body=tar_buffer.getvalue())\n",
    "    \n",
    "\n",
    "def upload_csv_to_s3(data, bucket_name, file_name):\n",
    "    buffer = StringIO()\n",
    "    data.to_csv(buffer, index=False)\n",
    "    s3_resource = boto3.resource('s3')\n",
    "    return s3_resource.Object(bucket_name, '{}.csv'.format(file_name)).put(Body=buffer.getvalue())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d9e1d736",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1/26 users (3.85%)\n",
      "Processed 2/26 users (7.69%)\n",
      "Processed 3/26 users (11.54%)\n",
      "Processed 4/26 users (15.38%)\n",
      "Processed 5/26 users (19.23%)\n",
      "Processed 6/26 users (23.08%)\n",
      "Processed 7/26 users (26.92%)\n",
      "Processed 8/26 users (30.77%)\n",
      "Processed 9/26 users (34.62%)\n",
      "Processed 10/26 users (38.46%)\n",
      "Processed 11/26 users (42.31%)\n",
      "Processed 12/26 users (46.15%)\n",
      "Processed 13/26 users (50.0%)\n",
      "Processed 14/26 users (53.85%)\n",
      "Processed 15/26 users (57.69%)\n",
      "Processed 16/26 users (61.54%)\n",
      "Processed 17/26 users (65.38%)\n",
      "Processed 18/26 users (69.23%)\n",
      "Processed 19/26 users (73.08%)\n",
      "Processed 20/26 users (76.92%)\n",
      "Processed 21/26 users (80.77%)\n",
      "Processed 22/26 users (84.62%)\n",
      "Processed 23/26 users (88.46%)\n",
      "Processed 24/26 users (92.31%)\n",
      "Processed 25/26 users (96.15%)\n",
      "Processed 26/26 users (100.0%)\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "test_data = extract_twitter_data(test_username)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a4f862bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'posts': [],\n",
       " 'following':             user        following      date\n",
       " 0   JoseRizal619       JoseMaBasa  04-18-22\n",
       " 1   JoseRizal619   LeonorRivera00  04-18-22\n",
       " 2   JoseRizal619  lopezantonio606  04-18-22\n",
       " 3   JoseRizal619       senorfabie  04-18-22\n",
       " 4   JoseRizal619      SaturninaMH  04-18-22\n",
       " 5   JoseRizal619    GovGenVWeyler  04-18-22\n",
       " 6   JoseRizal619  SilvestreUbaldo  04-18-22\n",
       " 7   JoseRizal619   LeonorRivera19  04-18-22\n",
       " 8   JoseRizal619   PacianoMercad0  04-18-22\n",
       " 9   JoseRizal619  WenceslaoRetana  04-18-22\n",
       " 10  JoseRizal619    SuzanneJacoby  04-18-22\n",
       " 11  JoseRizal619   AntonioLuna000  04-18-22\n",
       " 12  JoseRizal619          _dandoy  04-18-22\n",
       " 13  JoseRizal619  PlaridelMarcelo  04-18-22\n",
       " 14  JoseRizal619  _JosePanganiban  04-18-22\n",
       " 15  JoseRizal619    MarianoPonce5  04-18-22\n",
       " 16  JoseRizal619      AuntIsabel2  04-18-22\n",
       " 17  JoseRizal619  FrancisoMercado  04-18-22\n",
       " 18  JoseRizal619     TenantsniDon  04-18-22\n",
       " 19  JoseRizal619       platonnica  04-18-22\n",
       " 20  JoseRizal619   EduardBoustead  04-18-22\n",
       " 21  JoseRizal619  AdelinaBoustead  04-18-22\n",
       " 22  JoseRizal619    MisisBoustead  04-18-22\n",
       " 23  JoseRizal619  BousteadnNellie  04-18-22\n",
       " 24  JoseRizal619      mateoelorde  04-18-22,\n",
       " 'users':            id                  name         username\n",
       " 0   852182856   joseprotaciomercado     JoseRizal619\n",
       " 1   855967008         Jose Ma. Basa       JoseMaBasa\n",
       " 2   852500022         Leonor Rivera   LeonorRivera00\n",
       " 3   852176696         Antonio Lopez  lopezantonio606\n",
       " 4   852143006           Senor Fabie       senorfabie\n",
       " 5   852181310     Saturnina Hidalgo      SaturninaMH\n",
       " 6   852138301      Valeriano Weyler    GovGenVWeyler\n",
       " 7   852157788      Silvestre Ubaldo  SilvestreUbaldo\n",
       " 8   852350114         Leonor Rivera   LeonorRivera19\n",
       " 9   852337039        PacianoMercado   PacianoMercad0\n",
       " 10  852152900      Wenceslao Retana  WenceslaoRetana\n",
       " 11  852143474        Suzanne Jacoby    SuzanneJacoby\n",
       " 12  852135506          Antonio Luna   AntonioLuna000\n",
       " 13  852155508                dandoy          _dandoy\n",
       " 14  852126828  Marcelo H. del Pilar  PlaridelMarcelo\n",
       " 15  852163812   Jose Ma. Panganiban  _JosePanganiban\n",
       " 16  852192230         Mariano Ponce    MarianoPonce5\n",
       " 17  852181182           Aunt Isabel      AuntIsabel2\n",
       " 18  852086184     Francisco Mercado  FrancisoMercado\n",
       " 19  852167335               Tenants     TenantsniDon\n",
       " 20  852145902            Dominicano       platonnica\n",
       " 21  852176358        Eduard Bousted   EduardBoustead\n",
       " 22  852156392      Adelina Boustead  AdelinaBoustead\n",
       " 23  852136628        Misis Boustead    MisisBoustead\n",
       " 24  852205448       nellie boustead  BousteadnNellie\n",
       " 25  852148189    MateoteongElejorde      mateoelorde}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e2ae181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehend analysis\n",
    "\n",
    "def start_targeted_sentiment_job(input_s3_url, output_s3_url, job_tag):\n",
    "    input_data_config = {\n",
    "        'S3Uri': input_s3_url,\n",
    "        'InputFormat': input_doc_format\n",
    "    }\n",
    "\n",
    "    output_data_config = {\n",
    "        'S3Uri': output_s3_url\n",
    "    }\n",
    "\n",
    "    job_name = 'Targeted_Sentiment_Job_{}'.format(job_tag)\n",
    "    \n",
    "    comprehend = boto3.client('comprehend', region_name=region)\n",
    "    return comprehend.start_targeted_sentiment_detection_job(InputDataConfig=input_data_config,\n",
    "                                                             OutputDataConfig=output_data_config, \n",
    "                                                             DataAccessRoleArn=data_access_role_arn, \n",
    "                                                             LanguageCode=language_code,\n",
    "                                                             JobName=job_name)\n",
    "\n",
    "def analyse_tweets(username):\n",
    "    date = datetime.now().strftime(\"%m-%d-%y\")\n",
    "    tag = \"{}-{}\".format(date, username)\n",
    "    \n",
    "    twitter_data = extract_twitter_data(username)\n",
    "\n",
    "    posts = twitter_data['posts']\n",
    "    posts['line_id'] = posts.index.map(lambda x: '{}-{}'.format(x, tag)) # used for mapping entities\n",
    "\n",
    "    following = twitter_data['following']\n",
    "    users = twitter_data['users']\n",
    "    \n",
    "    session_folder = '{}/{}'.format(session_id, username)\n",
    "    tg_folder = '{}/{}'.format(tg_input_folder, session_folder) # Tigergraph files\n",
    "    comp_folder = '{}/{}'.format(comprehend_input_folder, session_folder) # Comprehend files\n",
    "\n",
    "    posts_filename = 'posts'\n",
    "    following_filename = 'following'\n",
    "    users_filename = 'users'\n",
    "    \n",
    "    # Upload data to Comprehend input folder\n",
    "    print(\"Uploading comprehend input files...\")\n",
    "    upload_text_to_s3(posts, input_bucket, '{}/{}_{}'.format(comp_folder, posts_filename, tag))\n",
    "    \n",
    "    print(\"Uploading Tigergraph input files...\")\n",
    "    # Upload data to Tigergraph input folder\n",
    "    uploaded_frames = {\n",
    "        f'{users_filename}.csv': users,\n",
    "        f'{following_filename}.csv': following,\n",
    "        f'{posts_filename}.csv': posts\n",
    "    }\n",
    "    upload_frames_to_s3(tg_folder, input_bucket, uploaded_frames)\n",
    "    \n",
    "    print(\"Starting comprehend job...\")\n",
    "    # Start comprehend job\n",
    "    input_s3_url = 's3://{}/{}'.format(input_bucket, comp_folder)\n",
    "    output_s3_url = 's3://{}/{}'.format(output_bucket, session_folder)\n",
    "    return start_targeted_sentiment_job(input_s3_url, output_s3_url, tag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "26ee03a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1/115 users (0.87%)\n",
      "Processed 2/115 users (1.74%)\n",
      "Processed 3/115 users (2.61%)\n",
      "Processed 4/115 users (3.48%)\n",
      "Processed 5/115 users (4.35%)\n",
      "Processed 6/115 users (5.22%)\n",
      "Processed 7/115 users (6.09%)\n",
      "Processed 8/115 users (6.96%)\n",
      "Processed 9/115 users (7.83%)\n",
      "Processed 10/115 users (8.7%)\n",
      "Processed 11/115 users (9.57%)\n",
      "Processed 12/115 users (10.43%)\n",
      "Processed 13/115 users (11.3%)\n",
      "Processed 14/115 users (12.17%)\n",
      "Processed 15/115 users (13.04%)\n",
      "Processed 16/115 users (13.91%)\n",
      "Processed 17/115 users (14.78%)\n",
      "Processed 18/115 users (15.65%)\n",
      "Processed 19/115 users (16.52%)\n",
      "Processed 20/115 users (17.39%)\n",
      "Processed 21/115 users (18.26%)\n",
      "Processed 22/115 users (19.13%)\n",
      "Processed 23/115 users (20.0%)\n",
      "Processed 24/115 users (20.87%)\n",
      "Processed 25/115 users (21.74%)\n",
      "Processed 26/115 users (22.61%)\n",
      "Processed 27/115 users (23.48%)\n",
      "Processed 28/115 users (24.35%)\n",
      "Processed 29/115 users (25.22%)\n",
      "Processed 30/115 users (26.09%)\n",
      "Processed 31/115 users (26.96%)\n",
      "Processed 32/115 users (27.83%)\n",
      "Processed 33/115 users (28.7%)\n",
      "Processed 34/115 users (29.57%)\n",
      "Processed 35/115 users (30.43%)\n",
      "Processed 36/115 users (31.3%)\n",
      "Processed 37/115 users (32.17%)\n",
      "Processed 38/115 users (33.04%)\n",
      "Processed 39/115 users (33.91%)\n",
      "Processed 40/115 users (34.78%)\n",
      "Processed 41/115 users (35.65%)\n",
      "Processed 42/115 users (36.52%)\n",
      "Processed 43/115 users (37.39%)\n",
      "Processed 44/115 users (38.26%)\n",
      "Processed 45/115 users (39.13%)\n",
      "Processed 46/115 users (40.0%)\n",
      "Processed 47/115 users (40.87%)\n",
      "Processed 48/115 users (41.74%)\n",
      "Processed 49/115 users (42.61%)\n",
      "Processed 50/115 users (43.48%)\n",
      "Processed 51/115 users (44.35%)\n",
      "Processed 52/115 users (45.22%)\n",
      "Processed 53/115 users (46.09%)\n",
      "Processed 54/115 users (46.96%)\n",
      "Processed 55/115 users (47.83%)\n",
      "Processed 56/115 users (48.7%)\n",
      "Processed 57/115 users (49.57%)\n",
      "Processed 58/115 users (50.43%)\n",
      "Processed 59/115 users (51.3%)\n",
      "Processed 60/115 users (52.17%)\n",
      "Processed 61/115 users (53.04%)\n",
      "Processed 62/115 users (53.91%)\n",
      "Processed 63/115 users (54.78%)\n",
      "Processed 64/115 users (55.65%)\n",
      "Processed 65/115 users (56.52%)\n",
      "Processed 66/115 users (57.39%)\n",
      "Processed 67/115 users (58.26%)\n",
      "Processed 68/115 users (59.13%)\n",
      "Processed 69/115 users (60.0%)\n",
      "Processed 70/115 users (60.87%)\n",
      "Processed 71/115 users (61.74%)\n",
      "Processed 72/115 users (62.61%)\n",
      "Processed 73/115 users (63.48%)\n",
      "Processed 74/115 users (64.35%)\n",
      "Processed 75/115 users (65.22%)\n",
      "Processed 76/115 users (66.09%)\n",
      "Processed 77/115 users (66.96%)\n",
      "Processed 78/115 users (67.83%)\n",
      "Processed 79/115 users (68.7%)\n",
      "Processed 80/115 users (69.57%)\n",
      "Processed 81/115 users (70.43%)\n",
      "Processed 82/115 users (71.3%)\n",
      "Processed 83/115 users (72.17%)\n",
      "Processed 84/115 users (73.04%)\n",
      "Processed 85/115 users (73.91%)\n",
      "Processed 86/115 users (74.78%)\n",
      "Processed 87/115 users (75.65%)\n",
      "Processed 88/115 users (76.52%)\n",
      "Processed 89/115 users (77.39%)\n",
      "Processed 90/115 users (78.26%)\n",
      "Processed 91/115 users (79.13%)\n",
      "Processed 92/115 users (80.0%)\n",
      "Processed 93/115 users (80.87%)\n",
      "Processed 94/115 users (81.74%)\n",
      "Processed 95/115 users (82.61%)\n",
      "Processed 96/115 users (83.48%)\n",
      "Processed 97/115 users (84.35%)\n",
      "Processed 98/115 users (85.22%)\n",
      "Processed 99/115 users (86.09%)\n",
      "Processed 100/115 users (86.96%)\n",
      "Processed 101/115 users (87.83%)\n",
      "Processed 102/115 users (88.7%)\n",
      "Processed 103/115 users (89.57%)\n",
      "Processed 104/115 users (90.43%)\n",
      "Processed 105/115 users (91.3%)\n",
      "Processed 106/115 users (92.17%)\n",
      "Processed 107/115 users (93.04%)\n",
      "Processed 108/115 users (93.91%)\n",
      "Processed 109/115 users (94.78%)\n",
      "Processed 110/115 users (95.65%)\n",
      "Processed 111/115 users (96.52%)\n",
      "Processed 112/115 users (97.39%)\n",
      "Processed 113/115 users (98.26%)\n",
      "Processed 114/115 users (99.13%)\n",
      "Processed 115/115 users (100.0%)\n",
      "Uploading comprehend input files...\n",
      "Uploading Tigergraph input files...\n",
      "Starting comprehend job...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'JobId': '4bdbfbdf964c2e06aaf70392c50572e7',\n",
       " 'JobArn': 'arn:aws:comprehend:ap-southeast-2:368767127050:targeted-sentiment-detection-job/4bdbfbdf964c2e06aaf70392c50572e7',\n",
       " 'JobStatus': 'SUBMITTED',\n",
       " 'ResponseMetadata': {'RequestId': '6b1ef4b3-b645-41ac-b05f-dc1cd034319a',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '6b1ef4b3-b645-41ac-b05f-dc1cd034319a',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '192',\n",
       "   'date': 'Sat, 16 Apr 2022 00:18:22 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyse_tweets(test_username)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
