{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6757cd5",
   "metadata": {},
   "source": [
    "# Marites Analyse\n",
    "\n",
    "## Overview\n",
    "Contains the logic for the analyse function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7da1212e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import complete.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import re\n",
    "import boto3\n",
    "from io import StringIO, BytesIO\n",
    "from uuid import uuid4\n",
    "import tarfile\n",
    "from tempfile import TemporaryDirectory\n",
    "\n",
    "load_dotenv()\n",
    "print(\"Import complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96f6fa42",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_twitter_posts = 100\n",
    "max_following = 250\n",
    "token = os.environ.get(\"BEARER_TOKEN\")\n",
    "test_username = 'elonmusk'\n",
    "\n",
    "region = 'ap-southeast-2'\n",
    "language_code = 'en'\n",
    "input_bucket = 'marites-comprehend-input'\n",
    "output_bucket = 'marites-comprehend-output'\n",
    "data_access_role_arn = os.environ.get(\"DATA_ACCESS_ROLE\")\n",
    "input_doc_format = 'ONE_DOC_PER_LINE'\n",
    "\n",
    "tg_input_folder = 'tigergraph' \n",
    "comprehend_input_folder = 'comprehend'\n",
    "\n",
    "session_id = uuid4()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45f83fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4602099d-bba9-4a12-b459-c79e484fb7e7\n"
     ]
    }
   ],
   "source": [
    "print(session_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e066e940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twitter Functions\n",
    "\n",
    "search_url = \"https://api.twitter.com/2/tweets/search/recent\"\n",
    "following_url = \"https://api.twitter.com/2/users/{}/following\"\n",
    "lookup_username_url = \"https://api.twitter.com/2/users/by/username/{}\"\n",
    "\n",
    "\n",
    "def bearer_oauth(r):\n",
    "    \"\"\"\n",
    "    Method required by bearer token authentication.\n",
    "    \"\"\"\n",
    "    r.headers[\"Authorization\"] = f\"Bearer {token}\"\n",
    "    return r\n",
    "\n",
    "\n",
    "def fetch_user_by_username(username):\n",
    "    url = lookup_username_url.format(username)\n",
    "    response = requests.get(url, auth=bearer_oauth)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(response.status_code, response.text)\n",
    "    json_res = response.json()\n",
    "    return json_res['data']\n",
    "\n",
    "def map_tweets_to_post(raw_data):\n",
    "    if 'data' not in raw_data:\n",
    "        return []\n",
    "\n",
    "    tweets = raw_data['data']\n",
    "    username = raw_data['includes']['users'][0]['username']\n",
    "    ref_tweets = { tweet['id']: tweet['text'] for tweet in raw_data['includes']['tweets'] } if 'includes' in raw_data and 'tweets' in raw_data['includes'] else {}\n",
    "    \n",
    "    results = []\n",
    "    for t in tweets:\n",
    "        post = { \n",
    "            'tweet_id': t['id'],\n",
    "            'username': username,\n",
    "            'created_at': t['created_at']\n",
    "        }\n",
    "        if 'referenced_tweets' in t:\n",
    "            combined_text = []\n",
    "            for rt in t['referenced_tweets']:\n",
    "                rt_id = rt['id']\n",
    "                if rt_id in ref_tweets:\n",
    "                    rt_text = ref_tweets[rt_id]\n",
    "                    combined_text.append(rt_text)\n",
    "            post['text'] = ' '.join(combined_text)\n",
    "        else:\n",
    "            post['text'] = t['text']\n",
    "\n",
    "        results.append(post)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def fetch_tweets_by_username(username):\n",
    "    params = {\n",
    "        \"query\": \"from:{} -is:reply\".format(username),\n",
    "        \"max_results\": max_twitter_posts,\n",
    "        \"expansions\": \"referenced_tweets.id,author_id\",\n",
    "        \"tweet.fields\": \"created_at\",\n",
    "        \"user.fields\": \"username\"\n",
    "    }\n",
    "    response = requests.get(search_url, auth=bearer_oauth, params=params)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(response.status_code, response.text)\n",
    "    data = response.json()\n",
    "    return map_tweets_to_post(data)\n",
    "\n",
    "def fetch_following(user_id):\n",
    "    url = following_url.format(user_id)\n",
    "    params = {\n",
    "        'max_results': max_following\n",
    "    }\n",
    "    response = requests.get(url, auth=bearer_oauth, params=params)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(response.status_code, response.text)\n",
    "    json_res = response.json()\n",
    "    return json_res['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f62234e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twitter data extraction\n",
    "\n",
    "def get_user_tweets(users_to_search):\n",
    "    processed = 0\n",
    "    all_tweets = []\n",
    "    for user in users_to_search:\n",
    "        user_tweets = fetch_tweets_by_username(user)\n",
    "        processed += 1\n",
    "        all_tweets.extend(user_tweets)\n",
    "        progress = round((processed / len(users_to_search)) * 100, 2)\n",
    "        print(\"Processed {}/{} users ({}%)\".format(processed, len(users_to_search), progress))\n",
    "    user_tweets = pd.DataFrame(all_tweets)\n",
    "    return user_tweets\n",
    "\n",
    "def get_user_following_map(user, following):\n",
    "    date = datetime.now().strftime(\"%m-%d-%y\")\n",
    "    username = user['username']\n",
    "    follow_names = list(map(lambda x: x['username'], following))\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        'user': [username] * len(following),\n",
    "        'following': follow_names,\n",
    "        'date': [date] * len(following)\n",
    "    })\n",
    "\n",
    "\n",
    "def clean_posts(data):\n",
    "    user_tweets = data\n",
    "    \n",
    "    # Clean up the links from the text (they're useless to us)\n",
    "    user_tweets['text'] = user_tweets['text'].apply(lambda x: re.split('https:\\/\\/.*', str(x))[0])\n",
    "\n",
    "    # Remove all emojis\n",
    "    user_tweets = user_tweets.astype(str).apply(lambda x: x.str.encode('ascii', 'ignore').str.decode('ascii'))\n",
    "\n",
    "    # Remove blank tweets\n",
    "    user_tweets = user_tweets[user_tweets.text.str.strip().str.len() != 0]\n",
    "\n",
    "    # Ensure that all text is in a single line\n",
    "    user_tweets.text = user_tweets.text.str.replace('\\n', ' ');\n",
    "    user_tweets.text = user_tweets.text.str.replace('\\r', ' ');\n",
    "    \n",
    "    return user_tweets\n",
    "\n",
    "def extract_twitter_data(username):\n",
    "    users_list = []\n",
    "    user = fetch_user_by_username(username)\n",
    "    user_following = fetch_following(user['id'])\n",
    "\n",
    "    users_list.append(user)\n",
    "    users_list.extend(user_following)\n",
    "    \n",
    "    users_to_search = list(map(lambda x: x['username'], users_list))\n",
    "    \n",
    "    posts_df = get_user_tweets(users_to_search)\n",
    "    following_df = get_user_following_map(user, user_following) # user -> following edges\n",
    "    users_df = pd.DataFrame(users_list) # users vertex\n",
    "    \n",
    "    return {\n",
    "        'posts': clean_posts(posts_df),\n",
    "        'following': following_df,\n",
    "        'users': users_df\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "36785355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1/114 users (0.88%)\n",
      "Processed 2/114 users (1.75%)\n",
      "Processed 3/114 users (2.63%)\n",
      "Processed 4/114 users (3.51%)\n",
      "Processed 5/114 users (4.39%)\n",
      "Processed 6/114 users (5.26%)\n",
      "Processed 7/114 users (6.14%)\n",
      "Processed 8/114 users (7.02%)\n",
      "Processed 9/114 users (7.89%)\n",
      "Processed 10/114 users (8.77%)\n",
      "Processed 11/114 users (9.65%)\n",
      "Processed 12/114 users (10.53%)\n",
      "Processed 13/114 users (11.4%)\n",
      "Processed 14/114 users (12.28%)\n",
      "Processed 15/114 users (13.16%)\n",
      "Processed 16/114 users (14.04%)\n",
      "Processed 17/114 users (14.91%)\n",
      "Processed 18/114 users (15.79%)\n",
      "Processed 19/114 users (16.67%)\n",
      "Processed 20/114 users (17.54%)\n",
      "Processed 21/114 users (18.42%)\n",
      "Processed 22/114 users (19.3%)\n",
      "Processed 23/114 users (20.18%)\n",
      "Processed 24/114 users (21.05%)\n",
      "Processed 25/114 users (21.93%)\n",
      "Processed 26/114 users (22.81%)\n",
      "Processed 27/114 users (23.68%)\n",
      "Processed 28/114 users (24.56%)\n",
      "Processed 29/114 users (25.44%)\n",
      "Processed 30/114 users (26.32%)\n",
      "Processed 31/114 users (27.19%)\n",
      "Processed 32/114 users (28.07%)\n",
      "Processed 33/114 users (28.95%)\n",
      "Processed 34/114 users (29.82%)\n",
      "Processed 35/114 users (30.7%)\n",
      "Processed 36/114 users (31.58%)\n",
      "Processed 37/114 users (32.46%)\n",
      "Processed 38/114 users (33.33%)\n",
      "Processed 39/114 users (34.21%)\n",
      "Processed 40/114 users (35.09%)\n",
      "Processed 41/114 users (35.96%)\n",
      "Processed 42/114 users (36.84%)\n",
      "Processed 43/114 users (37.72%)\n",
      "Processed 44/114 users (38.6%)\n",
      "Processed 45/114 users (39.47%)\n",
      "Processed 46/114 users (40.35%)\n",
      "Processed 47/114 users (41.23%)\n",
      "Processed 48/114 users (42.11%)\n",
      "Processed 49/114 users (42.98%)\n",
      "Processed 50/114 users (43.86%)\n",
      "Processed 51/114 users (44.74%)\n",
      "Processed 52/114 users (45.61%)\n",
      "Processed 53/114 users (46.49%)\n",
      "Processed 54/114 users (47.37%)\n",
      "Processed 55/114 users (48.25%)\n",
      "Processed 56/114 users (49.12%)\n",
      "Processed 57/114 users (50.0%)\n",
      "Processed 58/114 users (50.88%)\n",
      "Processed 59/114 users (51.75%)\n",
      "Processed 60/114 users (52.63%)\n",
      "Processed 61/114 users (53.51%)\n",
      "Processed 62/114 users (54.39%)\n",
      "Processed 63/114 users (55.26%)\n",
      "Processed 64/114 users (56.14%)\n",
      "Processed 65/114 users (57.02%)\n",
      "Processed 66/114 users (57.89%)\n",
      "Processed 67/114 users (58.77%)\n",
      "Processed 68/114 users (59.65%)\n",
      "Processed 69/114 users (60.53%)\n",
      "Processed 70/114 users (61.4%)\n",
      "Processed 71/114 users (62.28%)\n",
      "Processed 72/114 users (63.16%)\n",
      "Processed 73/114 users (64.04%)\n",
      "Processed 74/114 users (64.91%)\n",
      "Processed 75/114 users (65.79%)\n",
      "Processed 76/114 users (66.67%)\n",
      "Processed 77/114 users (67.54%)\n",
      "Processed 78/114 users (68.42%)\n",
      "Processed 79/114 users (69.3%)\n",
      "Processed 80/114 users (70.18%)\n",
      "Processed 81/114 users (71.05%)\n",
      "Processed 82/114 users (71.93%)\n",
      "Processed 83/114 users (72.81%)\n",
      "Processed 84/114 users (73.68%)\n",
      "Processed 85/114 users (74.56%)\n",
      "Processed 86/114 users (75.44%)\n",
      "Processed 87/114 users (76.32%)\n",
      "Processed 88/114 users (77.19%)\n",
      "Processed 89/114 users (78.07%)\n",
      "Processed 90/114 users (78.95%)\n",
      "Processed 91/114 users (79.82%)\n",
      "Processed 92/114 users (80.7%)\n",
      "Processed 93/114 users (81.58%)\n",
      "Processed 94/114 users (82.46%)\n",
      "Processed 95/114 users (83.33%)\n",
      "Processed 96/114 users (84.21%)\n",
      "Processed 97/114 users (85.09%)\n",
      "Processed 98/114 users (85.96%)\n",
      "Processed 99/114 users (86.84%)\n",
      "Processed 100/114 users (87.72%)\n",
      "Processed 101/114 users (88.6%)\n",
      "Processed 102/114 users (89.47%)\n",
      "Processed 103/114 users (90.35%)\n",
      "Processed 104/114 users (91.23%)\n",
      "Processed 105/114 users (92.11%)\n",
      "Processed 106/114 users (92.98%)\n",
      "Processed 107/114 users (93.86%)\n",
      "Processed 108/114 users (94.74%)\n",
      "Processed 109/114 users (95.61%)\n",
      "Processed 110/114 users (96.49%)\n",
      "Processed 111/114 users (97.37%)\n",
      "Processed 112/114 users (98.25%)\n",
      "Processed 113/114 users (99.12%)\n",
      "Processed 114/114 users (100.0%)\n"
     ]
    }
   ],
   "source": [
    "data = extract_twitter_data(test_username)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "76a5047c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>username</th>\n",
       "      <th>created_at</th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "      <th>line_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1512886651940491270</td>\n",
       "      <td>elonmusk</td>\n",
       "      <td>2022-04-09T20:14:20.000Z</td>\n",
       "      <td>69.420% of statistics are false</td>\n",
       "      <td>0-elonmusk</td>\n",
       "      <td>elonmusk-0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1512886157876600833</td>\n",
       "      <td>elonmusk</td>\n",
       "      <td>2022-04-09T20:12:22.000Z</td>\n",
       "      <td>Truth is the first casualty.</td>\n",
       "      <td>1-elonmusk</td>\n",
       "      <td>elonmusk-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1512813698011836422</td>\n",
       "      <td>elonmusk</td>\n",
       "      <td>2022-04-09T15:24:26.000Z</td>\n",
       "      <td>Thank you to everyone who came out to celebrat...</td>\n",
       "      <td>2-elonmusk</td>\n",
       "      <td>elonmusk-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1512787864458870787</td>\n",
       "      <td>elonmusk</td>\n",
       "      <td>2022-04-09T13:41:47.000Z</td>\n",
       "      <td>Docking confirmed!</td>\n",
       "      <td>3-elonmusk</td>\n",
       "      <td>elonmusk-3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1512785529712123906</td>\n",
       "      <td>elonmusk</td>\n",
       "      <td>2022-04-09T13:32:31.000Z</td>\n",
       "      <td>TOP 10 most followed Twitter accounts:    1. @...</td>\n",
       "      <td>4-elonmusk</td>\n",
       "      <td>elonmusk-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2512</th>\n",
       "      <td>1512405019013763076</td>\n",
       "      <td>SpaceX</td>\n",
       "      <td>2022-04-08T12:20:30.000Z</td>\n",
       "      <td>Ax-1 crew arrives at historic Launch Complex 3...</td>\n",
       "      <td>2512-elonmusk</td>\n",
       "      <td>elonmusk-2512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2513</th>\n",
       "      <td>1512398808537186304</td>\n",
       "      <td>SpaceX</td>\n",
       "      <td>2022-04-08T11:55:49.000Z</td>\n",
       "      <td>Watch Falcon 9 launch @Axiom_Spaces Ax-1 missi...</td>\n",
       "      <td>2513-elonmusk</td>\n",
       "      <td>elonmusk-2513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2514</th>\n",
       "      <td>1512360477988634625</td>\n",
       "      <td>SpaceX</td>\n",
       "      <td>2022-04-08T09:23:30.000Z</td>\n",
       "      <td>Targeting 11:17 a.m. ET for todays Falcon 9 la...</td>\n",
       "      <td>2514-elonmusk</td>\n",
       "      <td>elonmusk-2514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2515</th>\n",
       "      <td>1512065240116072466</td>\n",
       "      <td>SpaceX</td>\n",
       "      <td>2022-04-07T13:50:20.000Z</td>\n",
       "      <td>All systems are looking good for tomorrows Fal...</td>\n",
       "      <td>2515-elonmusk</td>\n",
       "      <td>elonmusk-2515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2516</th>\n",
       "      <td>1511784137861988356</td>\n",
       "      <td>SpaceX</td>\n",
       "      <td>2022-04-06T19:13:20.000Z</td>\n",
       "      <td>Static fire test of Falcon 9 complete  targeti...</td>\n",
       "      <td>2516-elonmusk</td>\n",
       "      <td>elonmusk-2516</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2350 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 tweet_id  username                created_at  \\\n",
       "0     1512886651940491270  elonmusk  2022-04-09T20:14:20.000Z   \n",
       "1     1512886157876600833  elonmusk  2022-04-09T20:12:22.000Z   \n",
       "2     1512813698011836422  elonmusk  2022-04-09T15:24:26.000Z   \n",
       "3     1512787864458870787  elonmusk  2022-04-09T13:41:47.000Z   \n",
       "4     1512785529712123906  elonmusk  2022-04-09T13:32:31.000Z   \n",
       "...                   ...       ...                       ...   \n",
       "2512  1512405019013763076    SpaceX  2022-04-08T12:20:30.000Z   \n",
       "2513  1512398808537186304    SpaceX  2022-04-08T11:55:49.000Z   \n",
       "2514  1512360477988634625    SpaceX  2022-04-08T09:23:30.000Z   \n",
       "2515  1512065240116072466    SpaceX  2022-04-07T13:50:20.000Z   \n",
       "2516  1511784137861988356    SpaceX  2022-04-06T19:13:20.000Z   \n",
       "\n",
       "                                                   text             id  \\\n",
       "0                       69.420% of statistics are false     0-elonmusk   \n",
       "1                         Truth is the first casualty.      1-elonmusk   \n",
       "2     Thank you to everyone who came out to celebrat...     2-elonmusk   \n",
       "3                                   Docking confirmed!      3-elonmusk   \n",
       "4     TOP 10 most followed Twitter accounts:    1. @...     4-elonmusk   \n",
       "...                                                 ...            ...   \n",
       "2512  Ax-1 crew arrives at historic Launch Complex 3...  2512-elonmusk   \n",
       "2513  Watch Falcon 9 launch @Axiom_Spaces Ax-1 missi...  2513-elonmusk   \n",
       "2514  Targeting 11:17 a.m. ET for todays Falcon 9 la...  2514-elonmusk   \n",
       "2515  All systems are looking good for tomorrows Fal...  2515-elonmusk   \n",
       "2516  Static fire test of Falcon 9 complete  targeti...  2516-elonmusk   \n",
       "\n",
       "            line_id  \n",
       "0        elonmusk-0  \n",
       "1        elonmusk-1  \n",
       "2        elonmusk-2  \n",
       "3        elonmusk-3  \n",
       "4        elonmusk-4  \n",
       "...             ...  \n",
       "2512  elonmusk-2512  \n",
       "2513  elonmusk-2513  \n",
       "2514  elonmusk-2514  \n",
       "2515  elonmusk-2515  \n",
       "2516  elonmusk-2516  \n",
       "\n",
       "[2350 rows x 6 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts = data['posts']\n",
    "posts['line_id'] = posts.index.map(lambda x: '{}-{}'.format(test_username, x))\n",
    "posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f7f90f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload to S3 functions\n",
    "\n",
    "def upload_text_to_s3(data, bucket_name, file_name):\n",
    "    text_buffer = StringIO()\n",
    "    data.text.to_csv(text_buffer, sep=' ', index=False, header=False)\n",
    "    s3_resource = boto3.resource('s3')\n",
    "    return s3_resource.Object(bucket_name, '{}.txt'.format(file_name)).put(Body=text_buffer.getvalue())\n",
    "\n",
    "def upload_frames_to_s3(tar_filename, bucket_name, frame_dict):\n",
    "    tar_buffer = BytesIO()\n",
    "    \n",
    "    # Create a tarfile into which frames can be added\n",
    "    with tarfile.open(fileobj=tar_buffer, mode='w:gz') as tfo:\n",
    "        \n",
    "        # Loop over all dataframes to be saved\n",
    "        for file_name, df in frame_dict.items():\n",
    "            \n",
    "            # Compute the full path of the output file within the archive\n",
    "            archive_name = os.path.join('output', file_name)\n",
    "            \n",
    "            # Create a temporary directory for packaging into a tar_file\n",
    "            with TemporaryDirectory(prefix='rev_processing__') as temp_dir:\n",
    "                \n",
    "                # Write a csv dump of the dataframe to a temporary file\n",
    "                temp_file_name = os.path.join(temp_dir, archive_name)\n",
    "                os.makedirs(os.path.dirname(temp_file_name), exist_ok=True)\n",
    "                df.to_csv(temp_file_name, index=False)\n",
    "                \n",
    "                # Add the temp file to the tarfile\n",
    "                tfo.add(temp_file_name, arcname=archive_name)\n",
    "    \n",
    "    # Upload to S3\n",
    "    s3_resource = boto3.resource('s3')\n",
    "    return s3_resource.Object(bucket_name, f'{tar_filename}.tar.gz').put(Body=tar_buffer.getvalue())\n",
    "    \n",
    "\n",
    "def upload_csv_to_s3(data, bucket_name, file_name):\n",
    "    buffer = StringIO()\n",
    "    data.to_csv(buffer, index=False)\n",
    "    s3_resource = boto3.resource('s3')\n",
    "    return s3_resource.Object(bucket_name, '{}.csv'.format(file_name)).put(Body=buffer.getvalue())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e2ae181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehend analysis\n",
    "\n",
    "def start_targeted_sentiment_job(input_s3_url, output_s3_url, job_tag):\n",
    "    input_data_config = {\n",
    "        'S3Uri': input_s3_url,\n",
    "        'InputFormat': input_doc_format\n",
    "    }\n",
    "\n",
    "    output_data_config = {\n",
    "        'S3Uri': output_s3_url\n",
    "    }\n",
    "\n",
    "    job_name = 'Targeted_Sentiment_Job_{}'.format(job_tag)\n",
    "    \n",
    "    comprehend = boto3.client('comprehend', region_name=region)\n",
    "    return comprehend.start_targeted_sentiment_detection_job(InputDataConfig=input_data_config,\n",
    "                                                             OutputDataConfig=output_data_config, \n",
    "                                                             DataAccessRoleArn=data_access_role_arn, \n",
    "                                                             LanguageCode=language_code,\n",
    "                                                             JobName=job_name)\n",
    "\n",
    "def analyse_tweets(username):\n",
    "    date = datetime.now().strftime(\"%m-%d-%y\")\n",
    "    tag = \"{}-{}\".format(date, username)\n",
    "    \n",
    "    twitter_data = extract_twitter_data(username)\n",
    "\n",
    "    posts = twitter_data['posts']\n",
    "    posts['line_id'] = posts.index.map(lambda x: '{}-{}'.format(x, tag)) # used for mapping entities\n",
    "\n",
    "    following = twitter_data['following']\n",
    "    users = twitter_data['users']\n",
    "    \n",
    "    session_folder = '{}/{}'.format(session_id, username)\n",
    "    tg_folder = '{}/{}'.format(tg_input_folder, session_folder) # Tigergraph files\n",
    "    comp_folder = '{}/{}'.format(comprehend_input_folder, session_folder) # Comprehend files\n",
    "\n",
    "    posts_filename = 'posts'\n",
    "    following_filename = 'following'\n",
    "    users_filename = 'users'\n",
    "    \n",
    "    # Upload data to Comprehend input folder\n",
    "    print(\"Uploading comprehend input files...\")\n",
    "    upload_text_to_s3(posts, input_bucket, '{}/{}_{}'.format(comp_folder, posts_filename, tag))\n",
    "    \n",
    "    print(\"Uploading Tigergraph input files...\")\n",
    "    # Upload data to Tigergraph input folder\n",
    "    uploaded_frames = {\n",
    "        f'{users_filename}.csv': users,\n",
    "        f'{following_filename}.csv': following,\n",
    "        f'{posts_filename}.csv': posts\n",
    "    }\n",
    "    upload_frames_to_s3(tg_folder, input_bucket, uploaded_frames)\n",
    "    \n",
    "    print(\"Starting comprehend job...\")\n",
    "    # Start comprehend job\n",
    "    input_s3_url = 's3://{}/{}'.format(input_bucket, comp_folder)\n",
    "    output_s3_url = 's3://{}/{}'.format(output_bucket, session_folder)\n",
    "    return start_targeted_sentiment_job(input_s3_url, output_s3_url, tag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26ee03a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1/115 users (0.87%)\n",
      "Processed 2/115 users (1.74%)\n",
      "Processed 3/115 users (2.61%)\n",
      "Processed 4/115 users (3.48%)\n",
      "Processed 5/115 users (4.35%)\n",
      "Processed 6/115 users (5.22%)\n",
      "Processed 7/115 users (6.09%)\n",
      "Processed 8/115 users (6.96%)\n",
      "Processed 9/115 users (7.83%)\n",
      "Processed 10/115 users (8.7%)\n",
      "Processed 11/115 users (9.57%)\n",
      "Processed 12/115 users (10.43%)\n",
      "Processed 13/115 users (11.3%)\n",
      "Processed 14/115 users (12.17%)\n",
      "Processed 15/115 users (13.04%)\n",
      "Processed 16/115 users (13.91%)\n",
      "Processed 17/115 users (14.78%)\n",
      "Processed 18/115 users (15.65%)\n",
      "Processed 19/115 users (16.52%)\n",
      "Processed 20/115 users (17.39%)\n",
      "Processed 21/115 users (18.26%)\n",
      "Processed 22/115 users (19.13%)\n",
      "Processed 23/115 users (20.0%)\n",
      "Processed 24/115 users (20.87%)\n",
      "Processed 25/115 users (21.74%)\n",
      "Processed 26/115 users (22.61%)\n",
      "Processed 27/115 users (23.48%)\n",
      "Processed 28/115 users (24.35%)\n",
      "Processed 29/115 users (25.22%)\n",
      "Processed 30/115 users (26.09%)\n",
      "Processed 31/115 users (26.96%)\n",
      "Processed 32/115 users (27.83%)\n",
      "Processed 33/115 users (28.7%)\n",
      "Processed 34/115 users (29.57%)\n",
      "Processed 35/115 users (30.43%)\n",
      "Processed 36/115 users (31.3%)\n",
      "Processed 37/115 users (32.17%)\n",
      "Processed 38/115 users (33.04%)\n",
      "Processed 39/115 users (33.91%)\n",
      "Processed 40/115 users (34.78%)\n",
      "Processed 41/115 users (35.65%)\n",
      "Processed 42/115 users (36.52%)\n",
      "Processed 43/115 users (37.39%)\n",
      "Processed 44/115 users (38.26%)\n",
      "Processed 45/115 users (39.13%)\n",
      "Processed 46/115 users (40.0%)\n",
      "Processed 47/115 users (40.87%)\n",
      "Processed 48/115 users (41.74%)\n",
      "Processed 49/115 users (42.61%)\n",
      "Processed 50/115 users (43.48%)\n",
      "Processed 51/115 users (44.35%)\n",
      "Processed 52/115 users (45.22%)\n",
      "Processed 53/115 users (46.09%)\n",
      "Processed 54/115 users (46.96%)\n",
      "Processed 55/115 users (47.83%)\n",
      "Processed 56/115 users (48.7%)\n",
      "Processed 57/115 users (49.57%)\n",
      "Processed 58/115 users (50.43%)\n",
      "Processed 59/115 users (51.3%)\n",
      "Processed 60/115 users (52.17%)\n",
      "Processed 61/115 users (53.04%)\n",
      "Processed 62/115 users (53.91%)\n",
      "Processed 63/115 users (54.78%)\n",
      "Processed 64/115 users (55.65%)\n",
      "Processed 65/115 users (56.52%)\n",
      "Processed 66/115 users (57.39%)\n",
      "Processed 67/115 users (58.26%)\n",
      "Processed 68/115 users (59.13%)\n",
      "Processed 69/115 users (60.0%)\n",
      "Processed 70/115 users (60.87%)\n",
      "Processed 71/115 users (61.74%)\n",
      "Processed 72/115 users (62.61%)\n",
      "Processed 73/115 users (63.48%)\n",
      "Processed 74/115 users (64.35%)\n",
      "Processed 75/115 users (65.22%)\n",
      "Processed 76/115 users (66.09%)\n",
      "Processed 77/115 users (66.96%)\n",
      "Processed 78/115 users (67.83%)\n",
      "Processed 79/115 users (68.7%)\n",
      "Processed 80/115 users (69.57%)\n",
      "Processed 81/115 users (70.43%)\n",
      "Processed 82/115 users (71.3%)\n",
      "Processed 83/115 users (72.17%)\n",
      "Processed 84/115 users (73.04%)\n",
      "Processed 85/115 users (73.91%)\n",
      "Processed 86/115 users (74.78%)\n",
      "Processed 87/115 users (75.65%)\n",
      "Processed 88/115 users (76.52%)\n",
      "Processed 89/115 users (77.39%)\n",
      "Processed 90/115 users (78.26%)\n",
      "Processed 91/115 users (79.13%)\n",
      "Processed 92/115 users (80.0%)\n",
      "Processed 93/115 users (80.87%)\n",
      "Processed 94/115 users (81.74%)\n",
      "Processed 95/115 users (82.61%)\n",
      "Processed 96/115 users (83.48%)\n",
      "Processed 97/115 users (84.35%)\n",
      "Processed 98/115 users (85.22%)\n",
      "Processed 99/115 users (86.09%)\n",
      "Processed 100/115 users (86.96%)\n",
      "Processed 101/115 users (87.83%)\n",
      "Processed 102/115 users (88.7%)\n",
      "Processed 103/115 users (89.57%)\n",
      "Processed 104/115 users (90.43%)\n",
      "Processed 105/115 users (91.3%)\n",
      "Processed 106/115 users (92.17%)\n",
      "Processed 107/115 users (93.04%)\n",
      "Processed 108/115 users (93.91%)\n",
      "Processed 109/115 users (94.78%)\n",
      "Processed 110/115 users (95.65%)\n",
      "Processed 111/115 users (96.52%)\n",
      "Processed 112/115 users (97.39%)\n",
      "Processed 113/115 users (98.26%)\n",
      "Processed 114/115 users (99.13%)\n",
      "Processed 115/115 users (100.0%)\n",
      "Uploading comprehend input files...\n",
      "Uploading Tigergraph input files...\n",
      "Starting comprehend job...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'JobId': '8f5d9a69eecf2f538c02c03c09383d40',\n",
       " 'JobArn': 'arn:aws:comprehend:ap-southeast-2:368767127050:targeted-sentiment-detection-job/8f5d9a69eecf2f538c02c03c09383d40',\n",
       " 'JobStatus': 'SUBMITTED',\n",
       " 'ResponseMetadata': {'RequestId': 'a5f9b447-ffa8-4cbe-ae59-2af3eec01b73',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'a5f9b447-ffa8-4cbe-ae59-2af3eec01b73',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '192',\n",
       "   'date': 'Fri, 15 Apr 2022 09:09:37 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyse_tweets(test_username)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
