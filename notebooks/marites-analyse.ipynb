{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6757cd5",
   "metadata": {},
   "source": [
    "# Marites Analyse\n",
    "\n",
    "## Overview\n",
    "Contains the logic for the analyse function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7da1212e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import complete.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import re\n",
    "import boto3\n",
    "from io import StringIO, BytesIO\n",
    "from uuid import uuid4\n",
    "import tarfile\n",
    "from tempfile import TemporaryDirectory\n",
    "\n",
    "load_dotenv()\n",
    "print(\"Import complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96f6fa42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5d187759-4a5d-4256-b02e-71c5020975ab\n"
     ]
    }
   ],
   "source": [
    "max_twitter_posts = 100\n",
    "max_following = 250\n",
    "token = os.environ.get(\"BEARER_TOKEN\")\n",
    "test_username = 'elonmusk'\n",
    "\n",
    "region = 'ap-southeast-2'\n",
    "language_code = 'en'\n",
    "input_bucket = 'marites-comprehend-input'\n",
    "output_bucket = 'marites-comprehend-output'\n",
    "data_access_role_arn = os.environ.get(\"DATA_ACCESS_ROLE\")\n",
    "input_doc_format = 'ONE_DOC_PER_LINE'\n",
    "\n",
    "tg_input_folder = 'tigergraph' \n",
    "comprehend_input_folder = 'comprehend'\n",
    "\n",
    "session_id = uuid4()\n",
    "print(session_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e066e940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twitter Functions\n",
    "\n",
    "search_url = \"https://api.twitter.com/2/tweets/search/recent\"\n",
    "following_url = \"https://api.twitter.com/2/users/{}/following\"\n",
    "lookup_username_url = \"https://api.twitter.com/2/users/by/username/{}\"\n",
    "\n",
    "\n",
    "def bearer_oauth(r):\n",
    "    \"\"\"\n",
    "    Method required by bearer token authentication.\n",
    "    \"\"\"\n",
    "    r.headers[\"Authorization\"] = f\"Bearer {token}\"\n",
    "    return r\n",
    "\n",
    "\n",
    "def fetch_user_by_username(username):\n",
    "    url = lookup_username_url.format(username)\n",
    "    response = requests.get(url, auth=bearer_oauth)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(response.status_code, response.text)\n",
    "    json_res = response.json()\n",
    "    return json_res['data']\n",
    "\n",
    "def map_tweets_to_post(raw_data):\n",
    "    if 'data' not in raw_data:\n",
    "        return []\n",
    "\n",
    "    tweets = raw_data['data']\n",
    "    username = raw_data['includes']['users'][0]['username']\n",
    "    ref_tweets = { tweet['id']: tweet['text'] for tweet in raw_data['includes']['tweets'] } if 'includes' in raw_data and 'tweets' in raw_data['includes'] else {}\n",
    "    \n",
    "    results = []\n",
    "    for t in tweets:\n",
    "        post = { \n",
    "            'tweet_id': t['id'],\n",
    "            'username': username,\n",
    "            'created_at': t['created_at']\n",
    "        }\n",
    "        if 'referenced_tweets' in t:\n",
    "            combined_text = []\n",
    "            for rt in t['referenced_tweets']:\n",
    "                rt_id = rt['id']\n",
    "                if rt_id in ref_tweets:\n",
    "                    rt_text = ref_tweets[rt_id]\n",
    "                    combined_text.append(rt_text)\n",
    "            post['text'] = ' '.join(combined_text)\n",
    "        else:\n",
    "            post['text'] = t['text']\n",
    "\n",
    "        results.append(post)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def fetch_tweets_by_username(username):\n",
    "    params = {\n",
    "        \"query\": \"from:{} -is:reply\".format(username),\n",
    "        \"max_results\": max_twitter_posts,\n",
    "        \"expansions\": \"referenced_tweets.id,author_id\",\n",
    "        \"tweet.fields\": \"created_at\",\n",
    "        \"user.fields\": \"username\"\n",
    "    }\n",
    "    response = requests.get(search_url, auth=bearer_oauth, params=params)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(response.status_code, response.text)\n",
    "    data = response.json()\n",
    "    return map_tweets_to_post(data)\n",
    "\n",
    "def fetch_following(user_id):\n",
    "    url = following_url.format(user_id)\n",
    "    params = {\n",
    "        'max_results': max_following\n",
    "    }\n",
    "    response = requests.get(url, auth=bearer_oauth, params=params)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(response.status_code, response.text)\n",
    "    json_res = response.json()\n",
    "    return json_res['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f62234e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twitter data extraction\n",
    "\n",
    "def get_user_tweets(users_to_search):\n",
    "    processed = 0\n",
    "    all_tweets = []\n",
    "    for user in users_to_search:\n",
    "        user_tweets = fetch_tweets_by_username(user)\n",
    "        processed += 1\n",
    "        all_tweets.extend(user_tweets)\n",
    "        progress = round((processed / len(users_to_search)) * 100, 2)\n",
    "        print(\"Processed {}/{} users ({}%)\".format(processed, len(users_to_search), progress))\n",
    "    user_tweets = pd.DataFrame(all_tweets)\n",
    "    return user_tweets\n",
    "\n",
    "def get_user_following_map(user, following):\n",
    "    date = datetime.now().strftime(\"%m-%d-%y\")\n",
    "    username = user['username']\n",
    "    follow_names = list(map(lambda x: x['username'], following))\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        'user': [username] * len(following),\n",
    "        'following': follow_names,\n",
    "        'date': [date] * len(following)\n",
    "    })\n",
    "\n",
    "\n",
    "def clean_posts(data):\n",
    "    user_tweets = data\n",
    "    \n",
    "    # Clean up the links from the text (they're useless to us)\n",
    "    user_tweets['text'] = user_tweets['text'].apply(lambda x: re.split('https:\\/\\/.*', str(x))[0])\n",
    "\n",
    "    # Remove all emojis\n",
    "    user_tweets = user_tweets.astype(str).apply(lambda x: x.str.encode('ascii', 'ignore').str.decode('ascii'))\n",
    "\n",
    "    # Remove blank tweets\n",
    "    user_tweets = user_tweets[user_tweets.text.str.strip().str.len() != 0]\n",
    "\n",
    "    # Ensure that all text is in a single line\n",
    "    user_tweets.text = user_tweets.text.str.replace('\\n', ' ');\n",
    "    user_tweets.text = user_tweets.text.str.replace('\\r', ' ');\n",
    "    \n",
    "    return user_tweets\n",
    "\n",
    "def extract_twitter_data(username):\n",
    "    users_list = []\n",
    "    user = fetch_user_by_username(username)\n",
    "    user_following = fetch_following(user['id'])\n",
    "\n",
    "    users_list.append(user)\n",
    "    users_list.extend(user_following)\n",
    "    \n",
    "    users_to_search = list(map(lambda x: x['username'], users_list))\n",
    "    \n",
    "    posts_df = get_user_tweets(users_to_search)\n",
    "    following_df = get_user_following_map(user, user_following) # user -> following edges\n",
    "    users_df = pd.DataFrame(users_list) # users vertex\n",
    "    \n",
    "    return {\n",
    "        'posts': clean_posts(posts_df),\n",
    "        'following': following_df,\n",
    "        'users': users_df\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f7f90f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload to S3 functions\n",
    "\n",
    "def upload_text_to_s3(data, bucket_name, file_name):\n",
    "    text_buffer = StringIO()\n",
    "    data.text.to_csv(text_buffer, sep=' ', index=False, header=False)\n",
    "    s3_resource = boto3.resource('s3')\n",
    "    return s3_resource.Object(bucket_name, '{}.txt'.format(file_name)).put(Body=text_buffer.getvalue())\n",
    "\n",
    "def upload_frames_to_s3(tar_filename, bucket_name, frame_dict):\n",
    "    tar_buffer = BytesIO()\n",
    "    \n",
    "    # Create a tarfile into which frames can be added\n",
    "    with tarfile.open(fileobj=tar_buffer, mode='w:gz') as tfo:\n",
    "        \n",
    "        # Loop over all dataframes to be saved\n",
    "        for file_name, df in frame_dict.items():\n",
    "            \n",
    "            # Compute the full path of the output file within the archive\n",
    "            archive_name = os.path.join('output', file_name)\n",
    "            \n",
    "            # Create a temporary directory for packaging into a tar_file\n",
    "            with TemporaryDirectory(prefix='rev_processing__') as temp_dir:\n",
    "                \n",
    "                # Write a csv dump of the dataframe to a temporary file\n",
    "                temp_file_name = os.path.join(temp_dir, archive_name)\n",
    "                os.makedirs(os.path.dirname(temp_file_name), exist_ok=True)\n",
    "                df.to_csv(temp_file_name, index=False)\n",
    "                \n",
    "                # Add the temp file to the tarfile\n",
    "                tfo.add(temp_file_name, arcname=archive_name)\n",
    "    \n",
    "    # Upload to S3\n",
    "    s3_resource = boto3.resource('s3')\n",
    "    return s3_resource.Object(bucket_name, f'{tar_filename}.tar.gz').put(Body=tar_buffer.getvalue())\n",
    "    \n",
    "\n",
    "def upload_csv_to_s3(data, bucket_name, file_name):\n",
    "    buffer = StringIO()\n",
    "    data.to_csv(buffer, index=False)\n",
    "    s3_resource = boto3.resource('s3')\n",
    "    return s3_resource.Object(bucket_name, '{}.csv'.format(file_name)).put(Body=buffer.getvalue())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e2ae181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehend analysis\n",
    "\n",
    "def start_targeted_sentiment_job(input_s3_url, output_s3_url, job_tag):\n",
    "    input_data_config = {\n",
    "        'S3Uri': input_s3_url,\n",
    "        'InputFormat': input_doc_format\n",
    "    }\n",
    "\n",
    "    output_data_config = {\n",
    "        'S3Uri': output_s3_url\n",
    "    }\n",
    "\n",
    "    job_name = 'Targeted_Sentiment_Job_{}'.format(job_tag)\n",
    "    \n",
    "    comprehend = boto3.client('comprehend', region_name=region)\n",
    "    return comprehend.start_targeted_sentiment_detection_job(InputDataConfig=input_data_config,\n",
    "                                                             OutputDataConfig=output_data_config, \n",
    "                                                             DataAccessRoleArn=data_access_role_arn, \n",
    "                                                             LanguageCode=language_code,\n",
    "                                                             JobName=job_name)\n",
    "\n",
    "def analyse_tweets(username):\n",
    "    date = datetime.now().strftime(\"%m-%d-%y\")\n",
    "    tag = \"{}-{}\".format(date, username)\n",
    "    \n",
    "    twitter_data = extract_twitter_data(username)\n",
    "\n",
    "    posts = twitter_data['posts']\n",
    "    posts['line_id'] = posts.index.map(lambda x: '{}-{}'.format(x, tag)) # used for mapping entities\n",
    "\n",
    "    following = twitter_data['following']\n",
    "    users = twitter_data['users']\n",
    "    \n",
    "    session_folder = '{}/{}'.format(session_id, username)\n",
    "    tg_folder = '{}/{}'.format(tg_input_folder, session_folder) # Tigergraph files\n",
    "    comp_folder = '{}/{}'.format(comprehend_input_folder, session_folder) # Comprehend files\n",
    "\n",
    "    posts_filename = 'posts'\n",
    "    following_filename = 'following'\n",
    "    users_filename = 'users'\n",
    "    \n",
    "    # Upload data to Comprehend input folder\n",
    "    print(\"Uploading comprehend input files...\")\n",
    "    upload_text_to_s3(posts, input_bucket, '{}/{}_{}'.format(comp_folder, posts_filename, tag))\n",
    "    \n",
    "    print(\"Uploading Tigergraph input files...\")\n",
    "    # Upload data to Tigergraph input folder\n",
    "    uploaded_frames = {\n",
    "        f'{users_filename}.csv': users,\n",
    "        f'{following_filename}.csv': following,\n",
    "        f'{posts_filename}.csv': posts\n",
    "    }\n",
    "    upload_frames_to_s3(tg_folder, input_bucket, uploaded_frames)\n",
    "    \n",
    "    print(\"Starting comprehend job...\")\n",
    "    # Start comprehend job\n",
    "    input_s3_url = 's3://{}/{}'.format(input_bucket, comp_folder)\n",
    "    output_s3_url = 's3://{}/{}'.format(output_bucket, session_folder)\n",
    "    return start_targeted_sentiment_job(input_s3_url, output_s3_url, tag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "26ee03a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1/115 users (0.87%)\n",
      "Processed 2/115 users (1.74%)\n",
      "Processed 3/115 users (2.61%)\n",
      "Processed 4/115 users (3.48%)\n",
      "Processed 5/115 users (4.35%)\n",
      "Processed 6/115 users (5.22%)\n",
      "Processed 7/115 users (6.09%)\n",
      "Processed 8/115 users (6.96%)\n",
      "Processed 9/115 users (7.83%)\n",
      "Processed 10/115 users (8.7%)\n",
      "Processed 11/115 users (9.57%)\n",
      "Processed 12/115 users (10.43%)\n",
      "Processed 13/115 users (11.3%)\n",
      "Processed 14/115 users (12.17%)\n",
      "Processed 15/115 users (13.04%)\n",
      "Processed 16/115 users (13.91%)\n",
      "Processed 17/115 users (14.78%)\n",
      "Processed 18/115 users (15.65%)\n",
      "Processed 19/115 users (16.52%)\n",
      "Processed 20/115 users (17.39%)\n",
      "Processed 21/115 users (18.26%)\n",
      "Processed 22/115 users (19.13%)\n",
      "Processed 23/115 users (20.0%)\n",
      "Processed 24/115 users (20.87%)\n",
      "Processed 25/115 users (21.74%)\n",
      "Processed 26/115 users (22.61%)\n",
      "Processed 27/115 users (23.48%)\n",
      "Processed 28/115 users (24.35%)\n",
      "Processed 29/115 users (25.22%)\n",
      "Processed 30/115 users (26.09%)\n",
      "Processed 31/115 users (26.96%)\n",
      "Processed 32/115 users (27.83%)\n",
      "Processed 33/115 users (28.7%)\n",
      "Processed 34/115 users (29.57%)\n",
      "Processed 35/115 users (30.43%)\n",
      "Processed 36/115 users (31.3%)\n",
      "Processed 37/115 users (32.17%)\n",
      "Processed 38/115 users (33.04%)\n",
      "Processed 39/115 users (33.91%)\n",
      "Processed 40/115 users (34.78%)\n",
      "Processed 41/115 users (35.65%)\n",
      "Processed 42/115 users (36.52%)\n",
      "Processed 43/115 users (37.39%)\n",
      "Processed 44/115 users (38.26%)\n",
      "Processed 45/115 users (39.13%)\n",
      "Processed 46/115 users (40.0%)\n",
      "Processed 47/115 users (40.87%)\n",
      "Processed 48/115 users (41.74%)\n",
      "Processed 49/115 users (42.61%)\n",
      "Processed 50/115 users (43.48%)\n",
      "Processed 51/115 users (44.35%)\n",
      "Processed 52/115 users (45.22%)\n",
      "Processed 53/115 users (46.09%)\n",
      "Processed 54/115 users (46.96%)\n",
      "Processed 55/115 users (47.83%)\n",
      "Processed 56/115 users (48.7%)\n",
      "Processed 57/115 users (49.57%)\n",
      "Processed 58/115 users (50.43%)\n",
      "Processed 59/115 users (51.3%)\n",
      "Processed 60/115 users (52.17%)\n",
      "Processed 61/115 users (53.04%)\n",
      "Processed 62/115 users (53.91%)\n",
      "Processed 63/115 users (54.78%)\n",
      "Processed 64/115 users (55.65%)\n",
      "Processed 65/115 users (56.52%)\n",
      "Processed 66/115 users (57.39%)\n",
      "Processed 67/115 users (58.26%)\n",
      "Processed 68/115 users (59.13%)\n",
      "Processed 69/115 users (60.0%)\n",
      "Processed 70/115 users (60.87%)\n",
      "Processed 71/115 users (61.74%)\n",
      "Processed 72/115 users (62.61%)\n",
      "Processed 73/115 users (63.48%)\n",
      "Processed 74/115 users (64.35%)\n",
      "Processed 75/115 users (65.22%)\n",
      "Processed 76/115 users (66.09%)\n",
      "Processed 77/115 users (66.96%)\n",
      "Processed 78/115 users (67.83%)\n",
      "Processed 79/115 users (68.7%)\n",
      "Processed 80/115 users (69.57%)\n",
      "Processed 81/115 users (70.43%)\n",
      "Processed 82/115 users (71.3%)\n",
      "Processed 83/115 users (72.17%)\n",
      "Processed 84/115 users (73.04%)\n",
      "Processed 85/115 users (73.91%)\n",
      "Processed 86/115 users (74.78%)\n",
      "Processed 87/115 users (75.65%)\n",
      "Processed 88/115 users (76.52%)\n",
      "Processed 89/115 users (77.39%)\n",
      "Processed 90/115 users (78.26%)\n",
      "Processed 91/115 users (79.13%)\n",
      "Processed 92/115 users (80.0%)\n",
      "Processed 93/115 users (80.87%)\n",
      "Processed 94/115 users (81.74%)\n",
      "Processed 95/115 users (82.61%)\n",
      "Processed 96/115 users (83.48%)\n",
      "Processed 97/115 users (84.35%)\n",
      "Processed 98/115 users (85.22%)\n",
      "Processed 99/115 users (86.09%)\n",
      "Processed 100/115 users (86.96%)\n",
      "Processed 101/115 users (87.83%)\n",
      "Processed 102/115 users (88.7%)\n",
      "Processed 103/115 users (89.57%)\n",
      "Processed 104/115 users (90.43%)\n",
      "Processed 105/115 users (91.3%)\n",
      "Processed 106/115 users (92.17%)\n",
      "Processed 107/115 users (93.04%)\n",
      "Processed 108/115 users (93.91%)\n",
      "Processed 109/115 users (94.78%)\n",
      "Processed 110/115 users (95.65%)\n",
      "Processed 111/115 users (96.52%)\n",
      "Processed 112/115 users (97.39%)\n",
      "Processed 113/115 users (98.26%)\n",
      "Processed 114/115 users (99.13%)\n",
      "Processed 115/115 users (100.0%)\n",
      "Uploading comprehend input files...\n",
      "Uploading Tigergraph input files...\n",
      "Starting comprehend job...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'JobId': '4bdbfbdf964c2e06aaf70392c50572e7',\n",
       " 'JobArn': 'arn:aws:comprehend:ap-southeast-2:368767127050:targeted-sentiment-detection-job/4bdbfbdf964c2e06aaf70392c50572e7',\n",
       " 'JobStatus': 'SUBMITTED',\n",
       " 'ResponseMetadata': {'RequestId': '6b1ef4b3-b645-41ac-b05f-dc1cd034319a',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '6b1ef4b3-b645-41ac-b05f-dc1cd034319a',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '192',\n",
       "   'date': 'Sat, 16 Apr 2022 00:18:22 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyse_tweets(test_username)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
