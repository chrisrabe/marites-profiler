{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd93e561",
   "metadata": {},
   "source": [
    "# Marites Proto\n",
    "\n",
    "## Overview\n",
    "This notebook contains the core functions used within the Marites API\n",
    "\n",
    "## Set up dependencies\n",
    "\n",
    "The lines below sets up the constants and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ea5b60ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import complete.\n"
     ]
    }
   ],
   "source": [
    "# All dependencies\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import re\n",
    "import boto3\n",
    "from io import StringIO\n",
    "\n",
    "load_dotenv()\n",
    "print('Import complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2a5a27ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_twitter_posts = 100\n",
    "max_following = 250\n",
    "token = os.environ.get(\"BEARER_TOKEN\")\n",
    "test_username = 'elonmusk'\n",
    "\n",
    "region = 'ap-southeast-1'\n",
    "language_code = 'en'\n",
    "input_bucket = 'marites-comprehend-input'\n",
    "output_bucket = 'marites-comprehend-output'\n",
    "data_access_role_arn = os.environ.get(\"DATA_ACCESS_ROLE\")\n",
    "input_doc_format = 'ONE_DOC_PER_LINE'\n",
    "\n",
    "session_id = '1' # we need to change this"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542882c5",
   "metadata": {},
   "source": [
    "## Lambda Functions\n",
    "\n",
    "**API Functions**\n",
    "\n",
    "- POST /analyse : Retrieves tweets, transforms raw data, places them into S3 bucket, triggers AWS Comprehend job\n",
    "- GET /user : Retrieves user from Tigergraph\n",
    "- GET /news : Retrieves news from News APIs\n",
    "\n",
    "**Internal Functions**\n",
    "\n",
    "- input_to_graph : Retrieves and parses the CSV from the input S3 bucket and pushes the data to Tigergraph\n",
    "- output_to_graph : Retrieves and parses the CSV from the output S3 bucket and pushes the data to Tigergraph\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4f6296",
   "metadata": {},
   "source": [
    "## Analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5db3a343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twitter Functions\n",
    "\n",
    "search_url = \"https://api.twitter.com/2/tweets/search/recent\"\n",
    "following_url = \"https://api.twitter.com/2/users/{}/following\"\n",
    "lookup_username_url = \"https://api.twitter.com/2/users/by/username/{}\"\n",
    "\n",
    "def bearer_oauth(r):\n",
    "    \"\"\"\n",
    "    Method required by bearer token authentication.\n",
    "    \"\"\"\n",
    "    r.headers[\"Authorization\"] = f\"Bearer {token}\"\n",
    "    return r\n",
    "\n",
    "def map_tweets_to_user(tweets, username):\n",
    "    if 'data' not in tweets:\n",
    "        return []\n",
    "    \n",
    "    ref_tweets = {tweet['id']: tweet['text'] for tweet in tweets['includes']['tweets']} if 'includes' in tweets else {}\n",
    "    raw_tweets = tweets['data']\n",
    "    \n",
    "    results = []\n",
    "    for t in raw_tweets:\n",
    "        result_tweet = { 'tweet_id': t['id'], 'username': username }\n",
    "        if 'referenced_tweets' in t:\n",
    "            combined_text = []\n",
    "            for rt in t['referenced_tweets']:\n",
    "                rt_id = rt['id']\n",
    "                if rt_id in ref_tweets:\n",
    "                    rt_text = ref_tweets[rt_id]\n",
    "                    combined_text.append(rt_text)\n",
    "            result_tweet['text'] = ' '.join(combined_text)\n",
    "        else:\n",
    "            result_tweet['text'] = t['text']\n",
    "        results.append(result_tweet)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def fetch_tweets_by_username(username):\n",
    "    params = {\n",
    "        \"query\": \"from:{} -is:reply\".format(username),\n",
    "        \"max_results\": max_twitter_posts,\n",
    "        \"expansions\": \"referenced_tweets.id\"\n",
    "    }\n",
    "    response = requests.get(search_url, auth=bearer_oauth, params=params)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(response.status_code, response.text)\n",
    "    data = response.json()\n",
    "    return map_tweets_to_user(data, username)\n",
    "\n",
    "def fetch_user_by_username(username):\n",
    "    url = lookup_username_url.format(username)\n",
    "    response = requests.get(url, auth=bearer_oauth)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(response.status_code, response.text)\n",
    "    json_res = response.json()\n",
    "    return json_res['data']\n",
    "\n",
    "def fetch_following_by_username(username):\n",
    "    user = fetch_user_by_username(username)\n",
    "    url = following_url.format(user['id'])\n",
    "    params = {\n",
    "        'max_results': max_following\n",
    "    }\n",
    "    response = requests.get(url, auth=bearer_oauth, params=params)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(response.status_code, response.text)\n",
    "    json_res = response.json()\n",
    "    return json_res['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "77465a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twitter data extraction\n",
    "\n",
    "def get_user_tweets(users_to_search):\n",
    "    processed = 0\n",
    "    all_tweets = []\n",
    "    for user in users_to_search:\n",
    "        user_tweets = fetch_tweets_by_username(user)\n",
    "        processed += 1\n",
    "        all_tweets.extend(user_tweets)\n",
    "        progress = round((processed / len(users_to_search)) * 100, 2)\n",
    "        print(\"Processed {}/{} users ({}%)\".format(processed, len(users_to_search), progress))\n",
    "    user_tweets = pd.DataFrame(all_tweets)\n",
    "    return user_tweets\n",
    "\n",
    "def get_user_following_map(user, following):\n",
    "    date = datetime.now().strftime(\"%m-%d-%y\")\n",
    "    return pd.DataFrame({\n",
    "        'user1': [user] * len(following),\n",
    "        'user2': following,\n",
    "        'date': [date] * len(following)\n",
    "    })\n",
    "\n",
    "def extract_twitter_data(username):\n",
    "    following_res = fetch_following_by_username(username)\n",
    "    user_following = list(map(lambda x: x['username'], following_res))\n",
    "\n",
    "    users_to_search = user_following.copy()\n",
    "    users_to_search.append(username)\n",
    "\n",
    "    user_tweets = get_user_tweets(users_to_search)\n",
    "    user_following_map = get_user_following_map(username, user_following)\n",
    "\n",
    "    return {\n",
    "        'user_tweets': user_tweets,\n",
    "        'user_following': user_following_map\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7b6c7c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehend analysis\n",
    "\n",
    "def prepare_data(data):\n",
    "    user_tweets = data\n",
    "    \n",
    "    # Clean up the links from the text (they're useless to us)\n",
    "    user_tweets['text'] = user_tweets['text'].apply(lambda x: re.split('https:\\/\\/.*', str(x))[0])\n",
    "\n",
    "    # Remove all emojis\n",
    "    user_tweets = user_tweets.astype(str).apply(lambda x: x.str.encode('ascii', 'ignore').str.decode('ascii'))\n",
    "\n",
    "    # Remove blank tweets\n",
    "    user_tweets = user_tweets[user_tweets.text.str.strip().str.len() != 0]\n",
    "\n",
    "    # Ensure that all text is in a single line\n",
    "    user_tweets.text = user_tweets.text.str.replace('\\n', ' ');\n",
    "    user_tweets.text = user_tweets.text.str.replace('\\r', ' ');\n",
    "    \n",
    "    return user_tweets\n",
    "\n",
    "def upload_to_s3(data, bucket_name, file_name):\n",
    "    text_buffer = StringIO()\n",
    "    data.text.to_csv(text_buffer, sep=' ', index=False, header=False)\n",
    "    s3_resource = boto3.resource('s3')\n",
    "    return s3_resource.Object(bucket_name, file_name).put(Body=text_buffer.getvalue())\n",
    "\n",
    "\n",
    "def start_targeted_sentiment_job(input_s3_url, output_s3_url):\n",
    "    input_data_config = {\n",
    "        'S3Uri': input_s3_url,\n",
    "        'InputFormat': input_doc_format\n",
    "    }\n",
    "\n",
    "    output_data_config = {\n",
    "        'S3Uri': output_s3_url\n",
    "    }\n",
    "\n",
    "    job_name = 'Targeted_Sentiment_Job_{}'.format(job_suffix)\n",
    "    \n",
    "    comprehend = boto3.client('comprehend', region_name=region)\n",
    "    return comprehend.start_targeted_sentiment_detection_job(InputDataConfig=input_data_config,\n",
    "                                                             OutputDataConfig=output_data_config, \n",
    "                                                             DataAccessRoleArn=data_access_role_arn, \n",
    "                                                             LanguageCode=language_code,\n",
    "                                                             JobName=job_name)\n",
    "\n",
    "def analyse_tweets(username):\n",
    "    # twitter_data <- Extract twitter data\n",
    "\n",
    "    # users CSV -> S3\n",
    "    # user_following CSV -> S3\n",
    "    # user_tweets CSV -> S3    \n",
    "    \n",
    "    # user_tweets TEXT -> S3\n",
    "\n",
    "    # Important!\n",
    "    #  - each tweet needs to be tagged with its line number"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
