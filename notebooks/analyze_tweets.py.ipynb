{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6485b99",
   "metadata": {},
   "source": [
    "# Analyze Tweets\n",
    "## Overview\n",
    "This notebook is used to create a prototype that uses AWS to perform NLP analysis on the output of the `extract-tweets` notebook. The purpose of this prototype is to generate an output that will be fed into Tigergraph to produce our model for a user's confirmation bias.\n",
    "\n",
    "## Set up dependencies\n",
    "Execute the line below to set up dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c404e281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: requests in /usr/local/lib/python3.9/site-packages (from -r requirements.txt (line 1)) (2.27.1)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.9/site-packages (from -r requirements.txt (line 2)) (1.4.1)\n",
      "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.9/site-packages (from -r requirements.txt (line 3)) (0.20.0)\n",
      "Requirement already satisfied: boto3 in /usr/local/lib/python3.9/site-packages (from -r requirements.txt (line 4)) (1.19.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/site-packages (from requests->-r requirements.txt (line 1)) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/site-packages (from requests->-r requirements.txt (line 1)) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/site-packages (from requests->-r requirements.txt (line 1)) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/site-packages (from requests->-r requirements.txt (line 1)) (1.26.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/site-packages (from pandas->-r requirements.txt (line 2)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/site-packages (from pandas->-r requirements.txt (line 2)) (2022.1)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.9/site-packages (from pandas->-r requirements.txt (line 2)) (1.22.3)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.9/site-packages (from boto3->-r requirements.txt (line 4)) (0.10.0)\n",
      "Requirement already satisfied: botocore<1.23.0,>=1.22.12 in /usr/local/lib/python3.9/site-packages (from boto3->-r requirements.txt (line 4)) (1.22.12)\n",
      "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /usr/local/lib/python3.9/site-packages (from boto3->-r requirements.txt (line 4)) (0.5.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->-r requirements.txt (line 2)) (1.16.0)\n",
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097f63e9",
   "metadata": {},
   "source": [
    "## Prepare our data\n",
    "\n",
    "We assume that the user-tweets.csv will be in our directory. If it's not present, it would be good to run the `extract-tweets` to generate the output file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12b72d18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>username</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1511297746661253120</td>\n",
       "      <td>thesheetztweetz</td>\n",
       "      <td>Breaking - Amazon $AMZN signed the biggest roc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1511153708654120963</td>\n",
       "      <td>thesheetztweetz</td>\n",
       "      <td>The U.S. Air Force's 388th Fighter Wing tested...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1511137391263715331</td>\n",
       "      <td>thesheetztweetz</td>\n",
       "      <td>U.S. Space Force Brig. Gen. Stephen Purdy rece...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1511087590832758789</td>\n",
       "      <td>thesheetztweetz</td>\n",
       "      <td>Due the vent valve issue, the launch director ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1510994152175149062</td>\n",
       "      <td>thesheetztweetz</td>\n",
       "      <td>The countdown clock has now resumed at T-6:40 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweet_id         username  \\\n",
       "1  1511297746661253120  thesheetztweetz   \n",
       "2  1511153708654120963  thesheetztweetz   \n",
       "3  1511137391263715331  thesheetztweetz   \n",
       "4  1511087590832758789  thesheetztweetz   \n",
       "5  1510994152175149062  thesheetztweetz   \n",
       "\n",
       "                                                text  \n",
       "1  Breaking - Amazon $AMZN signed the biggest roc...  \n",
       "2  The U.S. Air Force's 388th Fighter Wing tested...  \n",
       "3  U.S. Space Force Brig. Gen. Stephen Purdy rece...  \n",
       "4  Due the vent valve issue, the launch director ...  \n",
       "5  The countdown clock has now resumed at T-6:40 ...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load all user tweets\n",
    "user_tweets = pd.read_csv('./user-tweets.csv')\n",
    "\n",
    "# Drop index column attached to user tweets csv\n",
    "user_tweets = user_tweets.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "# Clean up the links from the text (they're useless to us)\n",
    "user_tweets['text'] = user_tweets['text'].apply(lambda x: re.split('https:\\/\\/.*', str(x))[0])\n",
    "\n",
    "# Remove all emojis\n",
    "user_tweets = user_tweets.astype(str).apply(lambda x: x.str.encode('ascii', 'ignore').str.decode('ascii'))\n",
    "\n",
    "# Remove blank tweets\n",
    "user_tweets = user_tweets[user_tweets.text.str.strip().str.len() != 0]\n",
    "\n",
    "# Ensure that all text is in a single line\n",
    "user_tweets.text = user_tweets.text.str.replace('\\n', ' ');\n",
    "user_tweets.text = user_tweets.text.str.replace('\\r', ' ');\n",
    "\n",
    "# Print out results\n",
    "user_tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b493b69",
   "metadata": {},
   "source": [
    "## Analyse tweets using AWS\n",
    "\n",
    "For our backend infrastructure, we'll be using AWS Comprehend as our machine learning component. It contains pre-trained models that can perform Key Phrase extraction, Sentiment analysis and Topic Modeling operations. Nothing custom required at this stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93b7597d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sounds of @inspiration4x DragonResilience during a phasing burn.I described in moment as an orchestra but its a more percussion-like rhythm &amp; very pleasant. So thankful for @SpaceX's talented team &amp; all the giants @NASA whose shoulders we stand on. @PolarisProgram up soon \n",
      "{'Sentiment': 'POSITIVE', 'SentimentScore': {'Positive': 0.9946348667144775, 'Negative': 0.000329022848745808, 'Neutral': 0.004913660231977701, 'Mixed': 0.00012249790597707033}, 'ResponseMetadata': {'RequestId': '44459cf0-e7f9-4c4c-8004-03746be6a3c5', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '44459cf0-e7f9-4c4c-8004-03746be6a3c5', 'content-type': 'application/x-amz-json-1.1', 'content-length': '165', 'date': 'Sat, 09 Apr 2022 01:00:26 GMT'}, 'RetryAttempts': 0}}\n",
      "{'KeyPhrases': [{'Score': 0.9979216456413269, 'Text': 'The sounds', 'BeginOffset': 0, 'EndOffset': 10}, {'Score': 0.9858503937721252, 'Text': '@inspiration4x DragonResilience', 'BeginOffset': 14, 'EndOffset': 45}, {'Score': 0.9908527135848999, 'Text': 'a phasing burn.I', 'BeginOffset': 53, 'EndOffset': 69}, {'Score': 0.9987554550170898, 'Text': 'moment', 'BeginOffset': 83, 'EndOffset': 89}, {'Score': 0.9997801184654236, 'Text': 'an orchestra', 'BeginOffset': 93, 'EndOffset': 105}, {'Score': 0.9792375564575195, 'Text': 'a more percussion-like rhythm', 'BeginOffset': 114, 'EndOffset': 143}, {'Score': 0.9491560459136963, 'Text': '@SpaceX', 'BeginOffset': 181, 'EndOffset': 188}, {'Score': 0.8190789818763733, 'Text': 'talented team', 'BeginOffset': 191, 'EndOffset': 204}, {'Score': 0.5180449485778809, 'Text': 'amp', 'BeginOffset': 206, 'EndOffset': 209}, {'Score': 0.9567962884902954, 'Text': 'all the giants', 'BeginOffset': 211, 'EndOffset': 225}, {'Score': 0.44274550676345825, 'Text': 'NASA', 'BeginOffset': 227, 'EndOffset': 231}, {'Score': 0.9446879625320435, 'Text': 'whose shoulders', 'BeginOffset': 232, 'EndOffset': 247}], 'ResponseMetadata': {'RequestId': '0d30a7fe-1737-48d6-8926-0ff132bf42f3', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '0d30a7fe-1737-48d6-8926-0ff132bf42f3', 'content-type': 'application/x-amz-json-1.1', 'content-length': '1043', 'date': 'Sat, 09 Apr 2022 01:00:27 GMT'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "region = 'ap-southeast-1'\n",
    "language_code = 'en'\n",
    "username = 'elonmusk'\n",
    "\n",
    "comprehend = boto3.client('comprehend', region_name=region)\n",
    "\n",
    "def detect_key_phrases(text, language_code):\n",
    "    response = comprehend.detect_key_phrases(Text=text, LanguageCode=language_code)\n",
    "    return response\n",
    "\n",
    "def detect_sentiment(text, language_code):\n",
    "    response = comprehend.detect_sentiment(Text=text, LanguageCode=language_code)\n",
    "    return response\n",
    "\n",
    "text = user_tweets.iloc[20].text\n",
    "\n",
    "sentiment = detect_sentiment(text, language_code)\n",
    "key_phrases = detect_key_phrases(text, language_code)\n",
    "entities = detect_entities(text, language_code)\n",
    "\n",
    "print(text)\n",
    "print(sentiment)\n",
    "print(key_phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49755f79",
   "metadata": {},
   "source": [
    "## Upload input data to S3\n",
    "\n",
    "This piece of code uploads data to our input S3 bucket. Make sure that the environment variable for `DATA_ACCESS_ROLE` is defined inside the `.env` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cb323d2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'Z1EJDSPTGF85BBP2',\n",
       "  'HostId': '/rwAABDz/v3bXee2AquXKUQpqm3cf3JxO6arZaUridluoewRFRSRQdpRf58YbK1veCr4ZtzvtSY=',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-id-2': '/rwAABDz/v3bXee2AquXKUQpqm3cf3JxO6arZaUridluoewRFRSRQdpRf58YbK1veCr4ZtzvtSY=',\n",
       "   'x-amz-request-id': 'Z1EJDSPTGF85BBP2',\n",
       "   'date': 'Sat, 09 Apr 2022 01:03:26 GMT',\n",
       "   'x-amz-expiration': 'expiry-date=\"Mon, 11 Apr 2022 00:00:00 GMT\", rule-id=\"comprehend-bucket-lifecycle\"',\n",
       "   'etag': '\"731e5065042c5e376fcecdd25b1b3fc2\"',\n",
       "   'server': 'AmazonS3',\n",
       "   'content-length': '0'},\n",
       "  'RetryAttempts': 0},\n",
       " 'Expiration': 'expiry-date=\"Mon, 11 Apr 2022 00:00:00 GMT\", rule-id=\"comprehend-bucket-lifecycle\"',\n",
       " 'ETag': '\"731e5065042c5e376fcecdd25b1b3fc2\"'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from io import StringIO\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "input_bucket = 'marites-comprehend-input'\n",
    "output_bucket = 'marites-comprehend-output'\n",
    "data_access_role_arn = os.environ.get(\"DATA_ACCESS_ROLE\")\n",
    "\n",
    "input_filename = '{}/{}-input.csv'.format(username, username)\n",
    "\n",
    "# convert data frame into buffer\n",
    "csv_buffer = StringIO()\n",
    "user_tweets.to_csv(csv_buffer)\n",
    "\n",
    "# upload data frame to S3\n",
    "s3_resource = boto3.resource('s3')\n",
    "s3_resource.Object(input_bucket, input_filename).put(Body=csv_buffer.getvalue())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cf54b3",
   "metadata": {},
   "source": [
    "## Topic Detection\n",
    "\n",
    "Analyses the tweets and splits them into 10 topics. Roughly takes ~30 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "488616b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "job_id: 637218a390140d1cc1f8ac78ef4dc8cd\n",
      "job_status: SUBMITTED\n",
      "job_status: IN_PROGRESS\n",
      "job_status: IN_PROGRESS\n",
      "job_status: IN_PROGRESS\n",
      "job_status: IN_PROGRESS\n",
      "job_status: IN_PROGRESS\n",
      "job_status: IN_PROGRESS\n",
      "job_status: IN_PROGRESS\n",
      "job_status: IN_PROGRESS\n",
      "job_status: IN_PROGRESS\n",
      "job_status: IN_PROGRESS\n",
      "job_status: IN_PROGRESS\n",
      "job_status: IN_PROGRESS\n",
      "job_status: IN_PROGRESS\n",
      "job_status: IN_PROGRESS\n",
      "job_status: IN_PROGRESS\n",
      "job_status: IN_PROGRESS\n",
      "job_status: IN_PROGRESS\n",
      "job_status: IN_PROGRESS\n",
      "job_status: IN_PROGRESS\n",
      "job_status: IN_PROGRESS\n",
      "job_status: IN_PROGRESS\n",
      "job_status: IN_PROGRESS\n",
      "job_status: IN_PROGRESS\n",
      "job_status: IN_PROGRESS\n",
      "job_status: IN_PROGRESS\n",
      "job_status: IN_PROGRESS\n",
      "job_status: IN_PROGRESS\n",
      "job_status: IN_PROGRESS\n",
      "job_status: IN_PROGRESS\n",
      "job_status: IN_PROGRESS\n",
      "job_status: COMPLETED\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "input_s3_url = 's3://{}/{}'.format(input_bucket, username)\n",
    "output_s3_url = 's3://{}/{}'.format(output_bucket, username)\n",
    "\n",
    "input_doc_format = 'ONE_DOC_PER_LINE'\n",
    "number_of_topics = 10\n",
    "\n",
    "input_data_config = {\n",
    "    'S3Uri': input_s3_url,\n",
    "    'InputFormat': input_doc_format\n",
    "}\n",
    "\n",
    "output_data_config = {\n",
    "    'S3Uri': output_s3_url\n",
    "}\n",
    "\n",
    "job_name = 'Topic_Analysis_Job_{}'.format(username)\n",
    "\n",
    "response = comprehend.start_topics_detection_job(NumberOfTopics=number_of_topics,\n",
    "                                                 InputDataConfig=input_data_config,\n",
    "                                                 OutputDataConfig=output_data_config, \n",
    "                                                 DataAccessRoleArn=data_access_role_arn, \n",
    "                                                 JobName=job_name)\n",
    "\n",
    "job_id = response['JobId']\n",
    "print('job_id: ' + job_id)\n",
    "\n",
    "while True:\n",
    "    result = comprehend.describe_topics_detection_job(JobId=job_id)\n",
    "    job_status = result[\"TopicsDetectionJobProperties\"][\"JobStatus\"]\n",
    "    \n",
    "    if job_status in ['COMPLETED', 'FAILED']:\n",
    "        print(\"job_status: \" + job_status)\n",
    "        break\n",
    "    else:\n",
    "        print(\"job_status: \" + job_status)\n",
    "        time.sleep(60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79493b60",
   "metadata": {},
   "source": [
    "## Download the data locally\n",
    "\n",
    "Download the topic modelling result locally so that we can analyse what we can do with the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4e4b2cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = comprehend.describe_topics_detection_job(JobId=job_id)\n",
    "results_s3_url = result['TopicsDetectionJobProperties']['OutputDataConfig']['S3Uri']\n",
    "\n",
    "\n",
    "s3_name = 's3://{}/'.format(output_bucket)\n",
    "\n",
    "local_results_filename = './topic-model.tar.gz'.format(username)\n",
    "results_aws_filename = results_s3_url.replace(s3_name, '')\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "s3.download_file(output_bucket, results_aws_filename, local_results_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6679d8ff",
   "metadata": {},
   "source": [
    "Extract the results into json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "17cf94c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "\n",
    "def extract_targz(targz_file, output_path=''):\n",
    "    if targz_file.endswith(\"tar.gz\"):\n",
    "        tar = tarfile.open(targz_file, \"r:gz\")\n",
    "        tar.extractall(path = output_path)\n",
    "        tar.close()\n",
    "    elif targz_file.endswith(\"tar\"):\n",
    "        tar = tarfile.open(targz_file, \"r:\")\n",
    "        tar.extractall(path=output_path)\n",
    "        tar.close()\n",
    "\n",
    "# Unzip the results file\n",
    "output_path = 'extracted'\n",
    "extract_targz(local_results_filename, output_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
